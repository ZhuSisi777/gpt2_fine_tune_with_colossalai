# Fine Tune GPT2 with Colossal AI
This example is reproducing the code from [Clossal-AI](https://github.com/hpcaitech/ColossalAI/blob/main/examples/language/gpt/hybridparallelism/finetune.py) on GPT2 fine tunning in distributed manners. 

## Introduction


## Requirements

Before you can launch training, you need to install the following requirements

## Dataset Used
The dataset used in this repository example is from [GLUE](https://huggingface.co/datasets/nyu-mll/glue), the General Language Understanding Evaluation benchmark. The task used is *mrpc*: the Microsoft Research Paraphrase Corpus. This corpus of sentence pairs, extracted from online news sources, with human annotations to flag whether the sentences pairs are semantically equivalent. 

## Result

